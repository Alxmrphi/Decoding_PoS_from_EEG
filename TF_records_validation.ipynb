{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords check\n",
    "\n",
    "This code checks whether the recovered / decoded data structure is the same data pre- and post-conversion to TFRecords format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF encode / decode functionality\n",
    "\n",
    "def _float_feature_seq(eeg_channel):\n",
    "    \"\"\" Convert sequence of EEG values to tf.train.FeatureList \"\"\"\n",
    "    feature_list = tf.train.FeatureList(feature=[\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(\n",
    "            value=eeg_channel))])\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "def _int_feature_scalar(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def encode_single_example(eeg_data: np.array, label: int):\n",
    "    \"\"\" TFRecords: process single EEG trial as a SequenceExample\"\"\"\n",
    "    #print(eeg_data.shape)\n",
    "    eeg_data_flat = eeg_data.copy().reshape((-1, 1))\n",
    "    \n",
    "    fl_dict = {'eeg_data':_float_feature_seq(eeg_data_flat)}\n",
    "    label_dict = {'label':  _int_feature_scalar(label)}\n",
    "    \n",
    "    label_features = tf.train.Features(feature=label_dict)\n",
    "    \n",
    "    #print(f'label_features : {label_features}')\n",
    "    \n",
    "    feature_lists = tf.train.FeatureLists(feature_list=fl_dict)\n",
    "    protobuff = tf.train.SequenceExample(context=label_features,\n",
    "                                         feature_lists=feature_lists)\n",
    "\n",
    "    protobuff_serialised = protobuff.SerializeToString()\n",
    "    \n",
    "    return protobuff_serialised\n",
    "\n",
    "def convert(data, labels, n_per_tfr, fname=None):\n",
    "    \"\"\" Convert NumPY EEG data to TFRecords\n",
    "    \n",
    "    Args:\n",
    "        data:      NumPy array of EEG data (shape = (n_batch, n_chan, n_time))\n",
    "        labels:    Accompanying labels for each row in `data`\n",
    "        n_per_tf:  How many EEG samples to include per TFRecord\n",
    "        fname:     Filename for saved TFRecords\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    max_range = (len(data) // n_per_tfr)\n",
    "    idx = [n_per_tfr * i for i in range(max_range+1)]\n",
    "    loop_idx = list(zip(idx, idx[1:]))\n",
    "\n",
    "    for i, (start, stop) in enumerate(loop_idx, start=1):\n",
    "        print(f'Iteration {i}/{len(loop_idx)}, start={start}, stop={stop}')\n",
    "\n",
    "        X = data[start:stop]\n",
    "        y = labels[start:stop]\n",
    "\n",
    "        file_path = f'{fname}_file{i}.tfrecords'\n",
    "\n",
    "        with tf.io.TFRecordWriter(file_path) as writer:\n",
    "            for sample, label in zip(X, y):\n",
    "                serialised_example = encode_single_example(sample, label)                              \n",
    "                writer.write(serialised_example)\n",
    "\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding function (NO Transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_example(serialised_example, START_WINDOW, END_WINDOW,\n",
    "                           TOTAL_TIMEPOINTS, N_ELECTRODES):\n",
    "\n",
    "    data_dim = TOTAL_TIMEPOINTS*N_ELECTRODES\n",
    "    #print(f'expected data_dim = {TOTAL_TIMEPOINTS}*64={data_dim}')\n",
    "\n",
    "    context_desc = {'label': tf.io.FixedLenFeature([], dtype=tf.int64)}\n",
    "    feature_desc = {\n",
    "        'eeg_data': tf.io.FixedLenSequenceFeature([data_dim], dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "    context, data = tf.io.parse_single_sequence_example(\n",
    "        serialized=serialised_example,\n",
    "        context_features=context_desc,\n",
    "        sequence_features=feature_desc,\n",
    "        name='parsing_single_seq_example')\n",
    "\n",
    "    data = data['eeg_data']\n",
    "    data = tf.reshape(data, (N_ELECTRODES, TOTAL_TIMEPOINTS))\n",
    "    #data = tf.transpose(data, (1,0))\n",
    "\n",
    "    data = data[:, START_WINDOW:END_WINDOW] # Extract (potential sub-window)\n",
    "    WINDOW_LENGTH = END_WINDOW - START_WINDOW\n",
    "    \n",
    "    # If using SVM and need to flatten again\n",
    "    #data = tf.reshape(data, (1, WINDOW_LENGTH * N_ELECTRODES))\n",
    "\n",
    "    label = context['label']\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords and return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files, BATCH_SIZE, repeat,\n",
    "               START_WINDOW, END_WINDOW, TOTAL_TIMEPOINTS, n_electrodes):\n",
    "    \n",
    "    decode_single_example_fn = partial(decode_single_example,\n",
    "                                       START_WINDOW=START_WINDOW,\n",
    "                                       END_WINDOW=END_WINDOW,\n",
    "                                       TOTAL_TIMEPOINTS=TOTAL_TIMEPOINTS,\n",
    "                                       N_ELECTRODES=n_electrodes)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=1)\n",
    "    dataset = dataset.map(decode_single_example_fn, num_parallel_calls=1)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    #dataset = dataset.shuffle(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build fake data & convert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_electrodes = 8\n",
    "n_timesteps = 5\n",
    "n_batch = 200\n",
    "\n",
    "data = np.zeros((n_batch, \n",
    "                n_electrodes,\n",
    "                n_timesteps))\n",
    "\n",
    "single_sample = np.zeros((n_electrodes,\n",
    "                         n_timesteps))\n",
    "\n",
    "for i in range(n_electrodes):\n",
    "    single_sample[i, :] = np.arange(n_timesteps)\n",
    "    \n",
    "for i in range(n_batch):\n",
    "    data[i,:] = single_sample\n",
    "\n",
    "labels = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each single sample of data is 5 timesteps over 8 channels (like if it were 8 EEG electrodes). Each channel just has the same value of increasing integers. This is the format that the real data is in as it is encoded (see the input docstring for the `convert` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/4, start=0, stop=50\n",
      "Iteration 2/4, start=50, stop=100\n",
      "Iteration 3/4, start=100, stop=150\n",
      "Iteration 4/4, start=150, stop=200\n"
     ]
    }
   ],
   "source": [
    "# convert data and put 50 samples in each tfrecord\n",
    "convert(data, labels, 50, fname=\"monday_testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recover all tfrecords and extract the dataset based on those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monday_testing_file1.tfrecords', 'monday_testing_file2.tfrecords', 'monday_testing_file3.tfrecords', 'monday_testing_file4.tfrecords', 'monday_testing_file5.tfrecords']\n"
     ]
    }
   ],
   "source": [
    "directory = pathlib.Path('.')\n",
    "files = list(directory.glob('monday*.tfrecords'))\n",
    "files = [str(x) for x in files]\n",
    "print(files)\n",
    "BATCH_SIZE = 1\n",
    "ds = get_dataset(files, BATCH_SIZE, 1, 0, n_timesteps, n_timesteps, n_electrodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]]], shape=(1, 8, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds:\n",
    "    if y.numpy() == 5:\n",
    "        print(x)\n",
    "        global test_sample\n",
    "        test_sample = x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we see, if we don't transpose, we recover the exact same information that went in. This is a good sign.\n",
    "\n",
    "What we want is to have the time dimension first, then the channel dimension, because this is how temporal data is fed to models in Tensorflow / Flax. For sklearn and EEG-library (which works with sklearn) it's with the temporal dimension last.\n",
    "\n",
    "So what we want the decoded output to look like is the transposed input, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [4., 4., 4., 4., 4., 4., 4., 4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we define a new decoding function where we uncomment out the transpose line, if we can recover the above, then we've got the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_example(serialised_example, START_WINDOW, END_WINDOW,\n",
    "                           TOTAL_TIMEPOINTS, N_ELECTRODES):\n",
    "\n",
    "    data_dim = TOTAL_TIMEPOINTS*N_ELECTRODES\n",
    "    #print(f'expected data_dim = {TOTAL_TIMEPOINTS}*64={data_dim}')\n",
    "\n",
    "    context_desc = {'label': tf.io.FixedLenFeature([], dtype=tf.int64)}\n",
    "    feature_desc = {\n",
    "        'eeg_data': tf.io.FixedLenSequenceFeature([data_dim], dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "    context, data = tf.io.parse_single_sequence_example(\n",
    "        serialized=serialised_example,\n",
    "        context_features=context_desc,\n",
    "        sequence_features=feature_desc,\n",
    "        name='parsing_single_seq_example')\n",
    "\n",
    "    data = data['eeg_data']\n",
    "    data = tf.reshape(data, (N_ELECTRODES, TOTAL_TIMEPOINTS))\n",
    "    data = tf.transpose(data, (1,0))\n",
    "\n",
    "    data = data[START_WINDOW:END_WINDOW, :] # Extract (potential sub-window)\n",
    "    WINDOW_LENGTH = END_WINDOW - START_WINDOW\n",
    "    \n",
    "    # If using SVM and need to flatten again\n",
    "    #data = tf.reshape(data, (1, WINDOW_LENGTH * N_ELECTRODES))\n",
    "\n",
    "    label = context['label']\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monday_testing_file1.tfrecords', 'monday_testing_file2.tfrecords', 'monday_testing_file3.tfrecords', 'monday_testing_file4.tfrecords', 'monday_testing_file5.tfrecords']\n"
     ]
    }
   ],
   "source": [
    "directory = pathlib.Path('.')\n",
    "files = list(directory.glob('monday*.tfrecords'))\n",
    "files = [str(x) for x in files]\n",
    "print(files)\n",
    "BATCH_SIZE = 1\n",
    "ds = get_dataset(files, BATCH_SIZE, 1, 0, n_timesteps, n_timesteps, n_electrodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "  [3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "  [4. 4. 4. 4. 4. 4. 4. 4.]]], shape=(1, 5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds:\n",
    "    if y.numpy() == 5:\n",
    "        print(x)\n",
    "        global test_sample\n",
    "        test_sample = x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoded sample we were taking as a checking example (5th sample in the fake dataset) matches exactly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
